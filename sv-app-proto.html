<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width">
    <title>SV App Prototype</title>
</head>
<body>

<audio src="./drums.ogg" preload="metadata" controls></audio>
<div id="track-1"></div>

</body>
<script src="./waves-ui.min.js"></script>
<script src="./Vamp.js"></script>
<script src="./VampUtils.js"></script>
<script>
    // Web Audio stuff using the MediaElementAudioNode for loading longer audio files
    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    const myAudio = document.querySelector('audio');
    const source = audioCtx.createMediaElementSource(myAudio);
    source.connect(audioCtx.destination);


    // app bootstrapping
    const vamp = VampJS({'onRuntimeInitialized': function () {
        const request = new XMLHttpRequest();
        request.open('GET', 'drums.ogg', true);
        request.responseType = 'arraybuffer';

        request.onload = () => {
            audioCtx.decodeAudioData(request.response).then((buffer) => {

                // wave-ui experimenting
                const trackDiv = document.querySelector('#track-1');
                const width = trackDiv.getBoundingClientRect().width;
                const height = 600;
                const duration = myAudio.duration;
                const pixelsPerSecond = width / duration;
                const timeline = new wavesUI.core.Timeline(pixelsPerSecond, width);
                timeline.createTrack(trackDiv, height, 'main');

                // time axis
                const timeAxis = new wavesUI.helpers.TimeAxisLayer({
                    height: height,
                    top: 10,
                    color: 'gray'
                });

                const cursorLayer = new wavesUI.helpers.CursorLayer({
                    height: 600
                });

                timeline.addLayer(timeAxis, 'main', 'default', true);
                timeline.addLayer(cursorLayer, 'main');
                timeline.state = new wavesUI.states.CenteredZoomState(timeline);

                // listen for time passing...
                (function updateSeekingCursor() {
                    cursorLayer.currentPosition = myAudio.currentTime == myAudio.duration ? 0 : myAudio.currentTime;
                    cursorLayer.update();

                    requestAnimationFrame(updateSeekingCursor);
                }());

                const feature = new VampPluginFeatureExtractor(vamp, vamp.createZeroCrossing(buffer.sampleRate)).extract(new AudioStream(buffer, vamp));
                const normalisationFactor = 1.0 / feature.data.value.reduce((currentMax, featureBin) => Math.max(currentMax, featureBin[0]), -Infinity);

                let normalisedFeatureData = feature.data.value.map((featureBin) => {
                    return featureBin.map((dataPoint) => dataPoint * normalisationFactor);
                });

                let plotData = [];
                for (let i = 0; i < feature.data.value.length; ++i) {
                    plotData.push({cx: feature.data.time[i], cy: normalisedFeatureData[i][0]});
                }

                // create the layer
                var breakpointLayer = new wavesUI.helpers.BreakpointLayer(plotData, {
                    height: height,
                    color: 'green'
                });

                timeline.addLayer(breakpointLayer, 'main');

                var waveformLayer = new wavesUI.helpers.WaveformLayer(buffer, {
                    height: 600,
                    color: 'darkblue'
                });
                timeline.addLayer(waveformLayer, 'main');
            }).catch(function (err) {
                console.log('Rendering failed: ' + err);
            });
        };
        request.send();
    }});

</script>
</html>